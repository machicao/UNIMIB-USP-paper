{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproch #1 threshold + intervaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#parameters \n",
    "#thre =np.linspace(0, 1, 11) #\n",
    "thre= [0.0, 0.1,   0.2,  0.3 ,0.4, 0.5, 0.6, 0.7, 0.8, 0.9 ]\n",
    "#thre= [0.0, 0.05, 0.1, 0.15, 0.2,0.25,  0.3,0.35 ,0.4,0.45, 0.5, 1.1]\n",
    "    \n",
    "reactions = pd.read_csv('ReactionMetabolites_list/R22_reactions.txt',sep=\"\\t\")\n",
    "metabolites= pd.read_csv('ReactionMetabolites_list/R22_metabolites.txt',sep=\"\\t\")\n",
    "#reading\n",
    "for class_label in ['normal', 'cancer']:\n",
    "    df = pd.read_csv('weightedNetworks/R22_'+class_label+'.txt',sep=\"\\t\").dropna() #remove rows with NaN #R22_normal\n",
    "    patients= df.columns.values\n",
    "    \n",
    "    #filtering by threshold\n",
    "    for i in range(0,len(thre)-1): \n",
    "        #selecting colunms \n",
    "        for col in range(2,107):#105 patients            \n",
    "            data_thre=df[(df.iloc[:,col] >= thre[i]) & (df.iloc[:,col] <thre[i+1])]\n",
    "            #writing\n",
    "            data_thre.iloc[:,[0,1]].to_csv('approaches/thre_interv/'+ class_label+'/'+patients[col]+'_th='+str(round(thre[i], 3))+'_'+str(round(thre[i+1], 3))+'.txt', sep='\\t', index=False,header=False) #, decimal=',')\n",
    "\n",
    "\n",
    "        #Filtering\n",
    "        # part1= dtemp[dtemp['NodeA'].isin(metabolites['ID'].values.tolist())] #columns on left with metabolites only\n",
    "        # part2= dtemp[dtemp['NodeA'].isin(reactions['ID'].values.tolist())] #columns on left with reactions only\n",
    "        # print(part1['NodeA'].size+part2['NodeA'].size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproch #2 threshold < than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#parameters \n",
    "#thre =np.linspace(0, 1, 11) #\n",
    "thre= [0.1,   0.2,  0.3 ,0.4, 0.5, 0.6, 0.7, 0.8, 0.9 ]\n",
    "    \n",
    "reactions = pd.read_csv('ReactionMetabolites_list/R22_reactions.txt',sep=\"\\t\")\n",
    "metabolites= pd.read_csv('ReactionMetabolites_list/R22_metabolites.txt',sep=\"\\t\")\n",
    "#reading\n",
    "for class_label in ['normal', 'cancer']:\n",
    "    df = pd.read_csv('weightedNetworks/R22_'+class_label+'.txt',sep=\"\\t\").dropna() #remove rows with NaN #R22_normal\n",
    "    patients= df.columns.values\n",
    "    \n",
    "    #filtering by threshold\n",
    "    for i in range(0,len(thre)-1): \n",
    "        #selecting colunms \n",
    "        for col in range(2,107):#105 patients            \n",
    "            data_thre=df[(df.iloc[:,col] <thre[i])]\n",
    "            #writing\n",
    "            data_thre.iloc[:,[0,1]].to_csv('approaches/thre_lessthan/'+ class_label+'/'+patients[col]+'_th='+str(round(thre[i], 3))+'.txt', sep='\\t', index=False,header=False) #, decimal=',')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save weighted networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#parameters  \n",
    "    \n",
    "reactions = pd.read_csv('ReactionMetabolites_list/R22_reactions.txt',sep=\"\\t\")\n",
    "metabolites= pd.read_csv('ReactionMetabolites_list/R22_metabolites.txt',sep=\"\\t\")\n",
    "#reading\n",
    "for class_label in ['normal', 'cancer']:\n",
    "    df = pd.read_csv('weightedNetworks/R22_'+class_label+'.txt',sep=\"\\t\").dropna() #remove rows with NaN #R22_normal\n",
    "    patients= df.columns.values\n",
    "    \n",
    "\n",
    "    #selecting colunms \n",
    "    for col in range(2,107):#105 patients      \n",
    "        #writing\n",
    "        df.iloc[:,[0,1,col]].to_csv('approaches/weigth/'+ class_label+'/'+patients[col]+'.txt', sep='\\t', index=False,header=False) #, decimal=',')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting network measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "from functionHierarchical import hierarchical_degree\n",
    "\n",
    "def tic():\n",
    "    #Homemade version of matlab tic and toc functions\n",
    "    import time\n",
    "    global startTime_for_tictoc\n",
    "    startTime_for_tictoc = time.time()\n",
    "\n",
    "def toc():\n",
    "    import time\n",
    "    if 'startTime_for_tictoc' in globals():\n",
    "        print(\"Elapsed time is \" + str(time.time() - startTime_for_tictoc) + \" seconds.\")\n",
    "    else:\n",
    "        print(\"Toc: start time not set\")\n",
    "        \n",
    "def extract_measures(G):\n",
    "    \"\"\"Extract a vector of measurements\n",
    "    \"\"\"\n",
    "    num_nodes =  nx.number_of_nodes(G)\n",
    "    if num_nodes == 0:\n",
    "        raise Exception('graph is empty')   \n",
    "    else: \n",
    "        F = {} #features \n",
    "\n",
    "        \"\"\" Global \"\"\"\n",
    "        H = hierarchical_degree(G, nivel_max=3)\n",
    "        F['avg_degree'] =  np.average(H[0,:])\n",
    "        F['avg_hier2']  =  np.average(H[1,:])\n",
    "        F['avg_hier3']  =  np.average(H[2,:])\n",
    "        #F['avg_clust']  = nx.average_clustering(G) #0, para este caso todas as redes obtiver 0\n",
    "        #tic()  #avgpathlenght is expensive\n",
    "        F['avg_path']   =  nx.algorithms.average_shortest_path_length(G)\n",
    "        F['pearson']    =  nx.algorithms.degree_pearson_correlation_coefficient(G) #-0.18656860948991077   \n",
    "        #F['avg_leff']   =  nx.local_efficiency(G) \n",
    "        #F['avg_geff']   =  nx.global_efficiency(G)\n",
    "        #F['avg_trans']  =  nx.algorithms.transitivity(G) #0\n",
    "\n",
    "        \"\"\" Local measures \"\"\"\n",
    "        #ecc  =   nx.algorithms.eccentricity(G)          # [0.01408451 0.   0.32394366 0.   0.66197183]\n",
    "        #tic() #betweeness is expensive\n",
    "        #bet  =   nx.algorithms.betweenness_centrality(G)# [0.97183099 0.   0.   0.01408451 0.01408451]\n",
    "        #katz =   nx.algorithms.katz_centrality(G)       # [0.67605634 0.29577465 0.    0.  0.02816901]\n",
    "        #clust  = nx.algorithms.clustering(G)           #[0. 0. 1. 0. 0.] O CLUSTERING NESTAS REDES DA O\n",
    "        #sqclust= nx.algorithms.square_clustering(G)    #[0.64788732 0.04225352 0.02816901 0.01408451 0.26760563]\n",
    "        #close  = nx.algorithms.closeness_centrality(G) #[0.64788732 0.32394366 0.    0.    0.02816901]  \n",
    "        #degcen = nx.algorithms.degree_centrality(G)    #[0.97183099 0.   0.     0.    0.02816901]\n",
    "\n",
    "\n",
    "        #F['avg_ecc']    = np.average(list(ecc.values()))\n",
    "        #F['avg_bet']    = np.average(list(bet.values()))    \n",
    "        #F['avg_katz']   = np.average(list(katz.values()))\n",
    "        #F['avg_sqclust']= np.average(list(sqclust.values()))\n",
    "        #F['avg_close']  = np.average(list(close.values()))\n",
    "        #F['avg_degcen'] = np.average(list(degcen.values()))\n",
    "\n",
    "        return F \n",
    "\n",
    "# def histo_descriptor(array, bins):\n",
    "#     hist, bin_edges = np.histogram(array, bins = bins)\n",
    "#     return hist/sum(hist) #density_histo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model as networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reactions = pd.read_csv('UNIMIB_data/ReactionMetabolites_list/R22_reactions.txt',sep=\"\\t\")\n",
    "metabolites= pd.read_csv('UNIMIB_data/ReactionMetabolites_list/R22_metabolites.txt',sep=\"\\t\")\n",
    "\n",
    "\n",
    "def read_network(filename): \n",
    "    \"\"\" return complete gcc network\"\"\"\n",
    "    df = pd.read_csv(filename,sep=\"\\t\")\n",
    "    nodeA= df.iloc[:,0]\n",
    "    nodeB= df.iloc[:,1]\n",
    "    peso = df.iloc[:,2]\n",
    "\n",
    "    all_nodes =pd.concat([nodeA, nodeB], ignore_index =True) #ignore_index, i.e re-indexa\n",
    "\n",
    "    G=nx.Graph() #DiGraph\n",
    "\n",
    "    #add nodes\n",
    "    for m in all_nodes[all_nodes.isin(metabolites['ID'])]:\n",
    "        #G.add_node(m, color='r') #metabolite    \n",
    "        G.add_node(m) #metabolite    \n",
    "    for r in all_nodes[~all_nodes.isin(metabolites['ID'])]:\n",
    "        #G.add_node(r, color='b') #reaction\n",
    "        G.add_node(r) #reaction\n",
    "    #add edges\n",
    "    for i in range(nodeA.size):\n",
    "        e1, e2, p=nodeA.get(i), nodeB.get(i), peso.get(i)\n",
    "        G.add_edge(e1, e2, peso=float(p))   \n",
    "    return G    \n",
    "    # largest connected component\n",
    "    #G0 = sorted(nx.connected_component_subgraphs(G), key=len, reverse=True)[0]#0 = the largest network\n",
    "    #return G0\n",
    "\n",
    "def threshold_network_range(G, thre_min, thre_max):\n",
    "    \"\"\" filtering by threshold \"\"\"\n",
    "    filtered=[(u,v) for (u,v,w) in G.edges(data=True) if (w['peso'] >=thre_min and w['peso'] < thre_max)]\n",
    "\n",
    "    if(len(list(filtered)) == 0):\n",
    "        raise Exception('filtered graph is empty')  \n",
    "    else: \n",
    "        # largest connected component\n",
    "        Gthre = sorted(nx.connected_component_subgraphs(nx.Graph(filtered)), key=len, reverse=True)[0]\n",
    "        #print('thre= [%.4f, %.4f], nodes= %d'% (thre_min, thre_max, nx.number_of_nodes(Gthre)))\n",
    "        return Gthre        \n",
    "    #write\n",
    "    # nx.write_graphml(G0_thre, 'TCGA_A7_A0CE_thre=0.7_0.8.graphml')  \n",
    "    \n",
    "def threshold_network_less_than(G, thre):\n",
    "    \"\"\" filtering by threshold \"\"\"\n",
    "    filtered=[(u,v) for (u,v,w) in G.edges(data=True) if (w['peso'] <= thre)]\n",
    "    if(len(list(filtered)) == 0):\n",
    "        raise Exception('filtered graph is empty')  \n",
    "    else: \n",
    "        # largest connected component\n",
    "        Gthre = sorted(nx.connected_component_subgraphs(nx.Graph(filtered)), key=len, reverse=True)[0]\n",
    "        #print('thre= [%.4f, %.4f], nodes= %d'% (thre_min, thre_max, nx.number_of_nodes(Gthre)))\n",
    "        return Gthre         \n",
    "    \n",
    "def threshold_network_bigger_than(G, thre):\n",
    "    \"\"\" filtering by threshold \"\"\"\n",
    "    filtered=[(u,v) for (u,v,w) in G.edges(data=True) if (w['peso'] >= thre)]\n",
    "\n",
    "    if(len(list(filtered)) == 0):\n",
    "        raise Exception('filtered graph is empty')  \n",
    "    else: \n",
    "        # largest connected component\n",
    "        Gthre = sorted(nx.connected_component_subgraphs(nx.Graph(filtered)), key=len, reverse=True)[0]\n",
    "        #print('thre= [%.4f, %.4f], nodes= %d'% (thre_min, thre_max, nx.number_of_nodes(Gthre)))\n",
    "        return Gthre        \n",
    "    #write\n",
    "    # nx.write_graphml(G0_thre, 'TCGA_A7_A0CE_thre=0.7_0.8.graphml')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G= read_network('approaches/weigth/normal/TCGA_A7_A0CE.txt')\n",
    "# nx.write_graphml(G, 'TCGA_A7_A0CE.graphml') \n",
    "print(nx.number_of_nodes(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "G= read_network('approaches/weigth/cancer/TCGA_A7_A0CE.txt')\n",
    "thre= [0.0, 10**-4, 10**-3, 10**-2, 0.1, 0.2,  0.3 ,0.4, 0.5,0.6, 1.1]\n",
    "for i in range(len(thre)-1):\n",
    "    Gthre=threshold_network_range(G, thre[i], thre[i+1])\n",
    "    #nx.write_graphml(G, 'TCGA_A7_A0CE.graphml') \n",
    "    #print('thre= [%.4f, %.4f], nodes= %d'% (thre[i], thre[i+1], nx.number_of_nodes(Gthre)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thre= 0.0001 , nodes= 514\n",
      "thre= 0.0010 , nodes= 683\n",
      "thre= 0.0100 , nodes= 1055\n",
      "thre= 0.1000 , nodes= 3289\n",
      "thre= 0.2000 , nodes= 5556\n",
      "thre= 0.3000 , nodes= 6662\n",
      "thre= 0.4000 , nodes= 7170\n",
      "thre= 0.5000 , nodes= 7699\n",
      "thre= 0.6000 , nodes= 7909\n",
      "thre= 0.7000 , nodes= 8103\n",
      "thre= 0.8000 , nodes= 8178\n",
      "thre= 0.9000 , nodes= 8202\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "G= read_network('approaches/weigth/cancer/TCGA_A7_A0CE.txt')\n",
    "thre= [10**-4, 10**-3, 10**-2, 0.1, 0.2,  0.3 ,0.4, 0.5,0.6, 0.7, 0.8, 0.9]\n",
    "for i in range(len(thre)):\n",
    "    #Gthre=threshold_network_bigger_than(G, thre[i])\n",
    "    Gthre=threshold_network_less_than(G, thre[i])\n",
    "    #nx.write_graphml(G, 'TCGA_A7_A0CE.graphml') \n",
    "    print('thre= %.4f , nodes= %d'% (thre[i], nx.number_of_nodes(Gthre)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# Load network\n",
    "\n",
    "#left-threshold\n",
    "# thre= [0.0, 10**-4, 10**-3, 10**-2, 0.1, 0.2,  0.3 ,0.4, 0.5,0.6, 1.1]   \n",
    "# thre= [0.0, 0.1, 0.2,  0.3 ,0.4, 0.5,0.6, 1.1]   \n",
    "# thre= [0.0, 10**-4, 10**-3]   \n",
    "\n",
    "#bigger_than threshold\n",
    "#thre= [10**-4, 10**-3, 10**-2, 0.1, 0.2,  0.3 ,0.4, 0.5,0.6, 0.7, 0.8, 0.9]\n",
    "thre= [10**-4, 10**-3, 10**-2, 0.1, 0.2,  0.3 ,0.4, 0.5,0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "pacientes= [f for f in listdir('approaches/weigth/cancer') if isfile(join('approaches/weigth/cancer', f))] \n",
    "#cancer has the same pacientes as normal\n",
    "\n",
    "path= 'approaches/weigth/'\n",
    "labels= ['cancer', 'normal']\n",
    "\n",
    "num_pacientes=len(pacientes)\n",
    "num_medidas=5 #'avg_degree', 'avg_hier2','avg_hier3','avg_path','pearson','avg_bet'\n",
    "#num_thre=(len(thre)-1) #Range threshold\n",
    "num_thre =len(thre)#>= threshold   \n",
    "num_descritores= num_thre * num_medidas\n",
    "\n",
    "features =  np.zeros((num_pacientes*2, num_descritores))\n",
    "k = list()\n",
    "str_clase=list()\n",
    "int_clase=list()\n",
    "atr_names= ['avg_degree', 'avg_hier2','avg_hier3','avg_path','pearson']*num_thre\n",
    "for c in range(len(labels)):\n",
    "    print(labels[c])\n",
    "    for p in range(num_pacientes):\n",
    "        G= read_network(path+labels[c]+'/'+ pacientes[p]) #reading \n",
    "        print(pacientes[p])\n",
    "        k.append(pacientes[p])\n",
    "        str_clase.append(labels[c])\n",
    "        int_clase.append(c)\n",
    "        #threshold      \n",
    "        #v=len(thre)-1 # #Range threshold\n",
    "        v= len(thre)  #>= threshold   \n",
    "        for i in range(v):\n",
    "            #Gthre=threshold_network_range(G, thre[i], thre[i+1]) # #Range threshold\n",
    "            #Gthre=threshold_network_bigger_than(G, thre[i]) #>= threshold     \n",
    "            Gthre=threshold_network_less_than(G, thre[i]) #<= threshold     \n",
    "            f=extract_measures(Gthre)  #dict \n",
    "            #print(f)\n",
    "            feats= list(f.values())\n",
    "            #atr_names= list(f.keys())\n",
    "                        \n",
    "            features[(c*num_pacientes)+p][i*num_medidas:(i*num_medidas)+num_medidas] = feats[:]             \n",
    "            #print('%s, thre= [%.4f, %.4f], %s'% (pacientes[p].replace('.txt',''), thre[i], thre[i+1],feats[:] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.DataFrame(features, columns=f.keys())\n",
    "data = pd.DataFrame(features)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(k)\n",
    "\n",
    "atr_names= ['avg_degree', 'avg_hier2','avg_hier3','avg_path','pearson']*num_thre\n",
    "# print(atr_names)\n",
    "data = pd.DataFrame(features, columns=atr_names)\n",
    "col1 = pd.DataFrame(k,columns= ['paciente'])\n",
    "col2 = pd.DataFrame(str_clase,columns= ['name_class'])\n",
    "col3 = pd.DataFrame(int_clase,columns= ['class'])\n",
    "\n",
    "df = pd.concat([data, col1['paciente'], col2['name_class'],col3['class']], axis = 1)\n",
    "df = pd.concat([data,   col3['class'] ], axis = 1)\n",
    "\n",
    "# df\n",
    "\n",
    "#save dataframe to csv \n",
    "\n",
    "df = pd.concat([data, col1['paciente'], col2['name_class'],col3['class']], axis = 1)\n",
    "#df.to_csv('Arffs/biggerthan.csv', sep='\\t', index=False)\n",
    "df.to_csv('Arffs/BOTAR.csv', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "#save dataframe to arff\n",
    "import arff\n",
    "df = pd.concat([data,   col3['class'] ], axis = 1)\n",
    "#arff.dump('biggerthan.arff', df.values, relation='thre=power', names=df.columns)\n",
    "arff.dump('BOTAR.arff', df.values, relation='thre=power', names=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Undirected and directed networks\n",
    "G=nx.Graph() \n",
    "\n",
    "G=nx.DiGraph()\n",
    "\n",
    "### identify largest connected component on Undirected\n",
    "Gcc = sorted(nx.connected_component_subgraphs(G), key=len, reverse=True)\n",
    "\n",
    "\n",
    "res= nx.algorithms.closeness_centrality(G0_thre)  \n",
    "\n",
    "### identigy largest component on Directed networks\n",
    "strongly_connected_component_subgraphs\n",
    "\n",
    "weakly_connected_component_subgraphs\n",
    "\n",
    "cent_indeg = nx.algorithms.in_degree_centrality(G0_thre) #not implemented for undirected type\n",
    "\n",
    "\n",
    "cent_outdeg= nx.algorithms.out_degree_centrality(G0_thre) #not implemented for undirected type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt \n",
    "\n",
    "# import networkx as nx\n",
    "\n",
    "# G=nx.Graph()\n",
    "# G.add_node('a', color='r')\n",
    "# G.add_node('b', color='b')\n",
    "# G.add_node('c', color='b')\n",
    "# G.add_node('d', color='r')\n",
    "# G.add_node('e', color='b')\n",
    "# G.add_node('f', color='r')\n",
    "\n",
    "\n",
    "\n",
    "# G.add_edge('a','b',weight=0.6)\n",
    "# G.add_edge('a','c',weight=0.2)\n",
    "# G.add_edge('c','d',weight=0.1)\n",
    "# G.add_edge('c','e',weight=0.7)\n",
    "# G.add_edge('c','f',weight=0.9)\n",
    "# G.add_edge('a','d',weight=0.3)\n",
    "\n",
    "# elarge=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] >0.5]\n",
    "# esmall=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] <=0.5]\n",
    "\n",
    "# pos=nx.spring_layout(G) # positions for all nodes\n",
    "# colores = [attrs['color'] for n, attrs in G.nodes(True)] #colors for all nodes\n",
    "\n",
    "# # nodes\n",
    "# nx.draw_networkx_nodes(G,pos,node_size=700,node_color=colores)\n",
    "# # colores = nx.get_node_attributes(G, 'color')  \n",
    "\n",
    "\n",
    "# # edges\n",
    "# nx.draw_networkx_edges(G,pos,edgelist=elarge,\n",
    "#                     width=6)\n",
    "# nx.draw_networkx_edges(G,pos,edgelist=esmall,\n",
    "#                     width=6,alpha=0.5,edge_color='b',style='dashed')\n",
    "\n",
    "# # labels\n",
    "# nx.draw_networkx_labels(G,pos,font_size=20,font_family='sans-serif')\n",
    " \n",
    "\n",
    "# plt.axis('off')\n",
    "# plt.savefig(\"weighted_graph.png\") # save as png\n",
    "# plt.show() # display\n",
    "\n",
    "# nx.write_graphml(G, 'TESTEE.graphml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize 2D Projection\n",
    "\n",
    "This section is just plotting 2 dimensional data. Notice on the graph below that the classes seem well separated from each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "#labels = [b'cancer', b'normal'] #cancer= 0 normal =1\n",
    "labels = ['cancer', 'normal'] \n",
    "targets=[0,1]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['class'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'PCA1']\n",
    "               , finalDf.loc[indicesToKeep, 'PCA2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(labels)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Projection to 2D from an arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# print(sklearn.__version__)\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA #from sklearn.lda import LDA\n",
    "\n",
    "# Abrir un archivo formato arff \n",
    "filename = \"/Volumes/GoogleDrive/Meu Drive/DOUTORADO/programas/BP-LLNA/resCancerDataset/classicalarffs/feats_classics.arff\"\n",
    " \n",
    "data = arff.loadarff(filename)\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "#features = ['d1','h2_1','H3_1','c1','l1','p1','d2','h2_2','H3_2','c2','...','c9','l9','p9','d10','h2_10','H3_10','c10','l10','p10']\n",
    "# Separating out the features\n",
    "x = df.iloc[:, 0:6*2].values #\n",
    "# Separating out the target\n",
    "y = df.loc[:,['class']].values\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['PCA1', 'PCA2'])\n",
    "\n",
    "finalDf = pd.concat([principalDf, df['class']], axis = 1)\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
